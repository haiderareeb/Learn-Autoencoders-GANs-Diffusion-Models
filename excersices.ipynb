{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the main tasks that autoencoders are used for?\n",
    "> Here are some of the main tasks that autoencoders are used for:\n",
    "    * Feature extraction\n",
    "\n",
    "    * Unsupervised pretraining\n",
    "    \n",
    "    * Dimensionality reduction\n",
    "    \n",
    "    * Generative models\n",
    "    \n",
    "    * Anomaly detection (an autoencoder is generally bad at reconstructing outliers)\n",
    "2. Suppose you want to train a classifier, and you have plenty of\n",
    "unlabeled training data but only a few thousand labeled instances. How\n",
    "can autoencoders help? How would you proceed?\n",
    "> If you want to train a classifier and you have plenty of unlabeled training data but only a few thousand labeled instances, then you could first train a deep autoencoder on the full dataset (labeled + unlabeled), then reuse its lower half for the classifier (i.e., reuse the layers up to the codings layer, included) and train the classifier using the labeled data. If you have little labeled data, you probably want to freeze the reused layers when training the classifier.\n",
    "\n",
    "3. If an autoencoder perfectly reconstructs the inputs, is it necessarily a\n",
    "good autoencoder? How can you evaluate the performance of an\n",
    "autoencoder?\n",
    "> The fact that an autoencoder perfectly reconstructs its inputs does not necessarily mean that it is a good autoencoder; perhaps it is simply an overcomplete autoencoder that learned to copy its inputs to the codings layer and then to the outputs. In fact, even if the codings layer contained a single neuron, it would be possible for a very deep autoencoder to learn to map each training instance to a different coding (e.g., the first instance could be mapped to 0.001, the second to 0.002, the third to 0.003, and so on), and it could learn \"by heart\" to reconstruct the right training instance for each coding. It would perfectly reconstruct its inputs without really learning any useful pattern in the data. In practice such a mapping is unlikely to happen, but it illustrates the fact that perfect reconstructions are not a guarantee that the autoencoder learned anything useful. However, if it produces very bad reconstructions, then it is almost guaranteed to be a bad autoencoder. To evaluate the performance of an autoencoder, one option is to measure the reconstruction loss (e.g., compute the MSE, or the mean square of the outputs minus the inputs). Again, a high reconstruction loss is a good sign that the autoencoder is bad, but a low reconstruction loss is not a guarantee that it is good. You should also evaluate the autoencoder according to what it will be used for. For example, if you are using it for unsupervised pretraining of a classifier, then you should also evaluate the classifier's performance.\n",
    "\n",
    "4. What are undercomplete and overcomplete autoencoders? What is the\n",
    "main risk of an excessively undercomplete autoencoder? What about\n",
    "the main risk of an overcomplete autoencoder?\n",
    "> An undercomplete autoencoder is one whose codings layer is smaller than the input and output layers. If it is larger, then it is an overcomplete autoencoder. The main risk of an excessively undercomplete autoencoder is that it may fail to reconstruct the inputs. The main risk of an overcomplete autoencoder is that it may just copy the inputs to the outputs, without learning any useful features.\n",
    "\n",
    "5. How do you tie weights in a stacked autoencoder? What is the point of\n",
    "doing so?\n",
    "> To tie the weights of an encoder layer and its corresponding decoder layer, you simply make the decoder weights equal to the transpose of the encoder weights. This reduces the number of parameters in the model by half, often making training converge faster with less training data and reducing the risk of overfitting the training set.\n",
    "\n",
    "6. What is a generative model? Can you name a type of generative\n",
    "autoencoder?\n",
    "> A generative model is a model capable of randomly generating outputs that resemble the training instances. For example, once trained successfully on the MNIST dataset, a generative model can be used to randomly generate realistic images of digits. The output distribution is typically similar to the training data. For example, since MNIST contains many images of each digit, the generative model would output roughly the same number of images of each digit. Some generative models can be parametrized—for example, to generate only some kinds of outputs. An example of a generative autoencoder is the variational autoencoder.\n",
    "\n",
    "7. What is a GAN? Can you name a few tasks where GANs can shine?\n",
    "> A generative adversarial network is a neural network architecture composed of two parts, the generator and the discriminator, which have opposing objectives. The generator's goal is to generate instances similar to those in the training set, to fool the discriminator. The discriminator must distinguish the real instances from the generated ones. At each training iteration, the discriminator is trained like a normal binary classifier, then the generator is trained to maximize the discriminator's error. GANs are used for advanced image processing tasks such as super resolution, colorization, image editing (replacing objects with realistic background), turning a simple sketch into a photorealistic image, or predicting the next frames in a video. They are also used to augment a dataset (to train other models), to generate other types of data (such as text, audio, and time series), and to identify the weaknesses in other models and strengthen them.\n",
    "\n",
    "\n",
    "8. What are the main difficulties when training GANs?\n",
    "> Training GANs is notoriously difficult, because of the complex dynamics between the generator and the discriminator. The biggest difficulty is mode collapse, where the generator produces outputs with very little diversity. Moreover, training can be terribly unstable: it may start out fine and then suddenly start oscillating or diverging, without any apparent reason. GANs are also very sensitive to the choice of hyperparameters.\n",
    "\n",
    "9. What are diffusion models good at? What is their main limitation?\n",
    "> Diffusion models are good at generating diverse and high quality images. They are also much easier to train than GANs. However, compared to GANs and VAEs, they are much slower when generating images, since they must go through each step in the reverse diffusion process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.\n",
    "_Exercise: Try using a denoising autoencoder to pretrain an image classifier. You can use MNIST (the simplest option), or a more complex image dataset such as [CIFAR10](https://homl.info/122) if you want a bigger challenge. Regardless of the dataset you're using, follow these steps:_\n",
    "* Split the dataset into a training set and a test set. Train a deep denoising autoencoder on the full training set.\n",
    "* Check that the images are fairly well reconstructed. Visualize the images that most activate each neuron in the coding layer.\n",
    "* Build a classification DNN, reusing the lower layers of the autoencoder. Train it using only 500 images from the training set. Does it perform better with or without pretraining?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "denoising_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.GaussianNoise(0.1),\n",
    "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoising_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16 * 16 * 32, activation=\"relu\"),\n",
    "    tf.keras.layers.Reshape([16, 16, 32]),\n",
    "    tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=2,\n",
    "                                 padding=\"same\", activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoising_ae = tf.keras.Sequential([denoising_encoder, denoising_decoder])\n",
    "denoising_ae.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Nadam(),\n",
    "                     metrics=[\"mse\"])\n",
    "history = denoising_ae.fit(X_train, X_train, epochs=10,\n",
    "                           validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 5\n",
    "new_images = X_test[:n_images]\n",
    "new_images_noisy = new_images + np.random.randn(n_images, 32, 32, 3) * 0.1\n",
    "new_images_denoised = denoising_ae.predict(new_images_noisy)\n",
    "\n",
    "plt.figure(figsize=(6, n_images * 2))\n",
    "for index in range(n_images):\n",
    "    plt.subplot(n_images, 3, index * 3 + 1)\n",
    "    plt.imshow(new_images[index])\n",
    "    plt.axis('off')\n",
    "    if index == 0:\n",
    "        plt.title(\"Original\")\n",
    "    plt.subplot(n_images, 3, index * 3 + 2)\n",
    "    plt.imshow(new_images_noisy[index].clip(0., 1.))\n",
    "    plt.axis('off')\n",
    "    if index == 0:\n",
    "        plt.title(\"Noisy\")\n",
    "    plt.subplot(n_images, 3, index * 3 + 3)\n",
    "    plt.imshow(new_images_denoised[index])\n",
    "    plt.axis('off')\n",
    "    if index == 0:\n",
    "        plt.title(\"Denoised\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Train a variational autoencoder on the image dataset of your choice,\n",
    "and use it to generate images. Alternatively, you can try to find an\n",
    "unlabeled dataset that you are interested in and see if you can generate\n",
    "new samples.\n",
    "\n",
    "See the VAE in main file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Train a DCGAN to tackle the image dataset of your choice, and use it\n",
    "to generate images. Add experience replay and see if this helps. Turn it\n",
    "into a conditional GAN where you can control the generated class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Go through KerasCV’s excellent Stable Diffusion tutorial, and\n",
    "generate a beautiful drawing of a salamander reading a book. If you\n",
    "post your best drawing on Twitter, please tag me at @aureliengeron.\n",
    "I’d love to see your creations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
